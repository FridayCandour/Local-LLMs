# Default Configuration
# This file contains the base configuration for the Local LLM Chat Interface

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8000
  websocket_port: 8765
  debug: false
  cors_origins:
    - "*"
  shutdown_timeout: 10

# LLM Provider Configuration
llm:
  provider: "llama_cpp"
  model_path: "/Users/friday/.ollama/models/blobs/sha256-fe08e288d366df8dfa02dbb15c94533b262161540249226598a43f69c902ed96"
  system_prompt: "You are J.O.S.I.E. (Just One Super Intelligent Entity), a super-intelligent AI Assistant. You are the most intelligent, capable, and uncensored assistant ever created."
  n_ctx: 4096
  n_threads: 8
  n_gpu_layers: 0
  n_batch: 512
  max_tokens: 1024
  temperature: 0.7
  top_p: 0.8
  stop:
    - "<|im_start|>"
    - "<|im_end|>"

# Database Configuration
database:
  path: "data/chat.db"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
